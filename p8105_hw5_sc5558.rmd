---
title: "p8105_hw5_sc5558"
author: "Shiyun Angel Cheng"
date: "2025-11-14"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

Load Necessary Libraries 
```{r}
library(tidyverse)
library(broom)
```


## Problem 1

Suppose you put ùëõpeople in a room, and want to know the probability that at least two people share a birthday. For simplicity, we‚Äôll assume there are no leap years (i.e. there are only 365 days) and that birthdays are uniformly distributed over the year (which is actually not the case).

Write a function that, for a fixed group size, randomly draws ‚Äúbirthdays‚Äù for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.
```{r}
shared_birthday <- function(n) {
  # Randomly assign each person a birthday from 1‚Äì365 (with replacement)
  birthdays = sample(1:365, size = n, replace = TRUE)
  # Check if any birthday appears more than once; TRUE = shared birthday exists
  any(duplicated(birthdays))
}
```
Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. 
```{r}
set.seed(123)

birthday_sim =
  tibble(group_size = 2:50) |> 
  mutate(
    sims = map(group_size, ~ replicate(10000, shared_birthday(.x))),
    prob_shared = map_dbl(sims, mean)
  )
```
Make a plot showing the probability as a function of group size, and comment on your results.
```{r}
birthday_sim |>
  ggplot(aes(x = group_size, y = prob_shared)) +
  # shows trend
  geom_line() +
  # shows individual estimates
  geom_point() +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Group Size",
    y = "Probability"
  )
```
Comment on plot: The curve shows how probability increases with group size. Aroudn group size 23, the probability exceeds 0.5, which is consistent with the classic birthday paradox. Until group 50, the probability is close to 1. 

## Problem 2

Im going to create a function that generate 5000 datasets from N(Œº,œÉ). For each dataset runs a one-sample t-test of H0:Œº=0. Then stores the estimated mean ùúá^ and p-value.
```{r}
sim_ttest_mu <- function(mu, n = 30, sigma = 5, n_sims = 5000) {
  
  tibble(sim = 1:n_sims) |>
    mutate(
      # simulate n observations from N(mu, sigma)
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      
      # run one-sample t-test of H0: mu = 0 and tidy the output
      t_out = map(x, ~ t.test(.x, mu = 0) |> tidy()),
      
      # pull out estimate and p-value
      mu_hat  = map_dbl(t_out, "estimate"),
      p_value = map_dbl(t_out, "p.value"),
      
    )
}
```
Run simulations for Œº = 0, 1, 2, 3, 4, 5, 6
```{r}
set.seed(123)

sim_results =
  tibble(true_mu = 0:6) |>
  mutate(
    sim_df = map(true_mu, sim_ttest_mu)
  ) |>
  unnest(sim_df)
```
(a) Power vs. effect size Œº
Compute the proportion of rejections of each Œº: 
```{r}
power_df =
  sim_results |>
  group_by(true_mu) |>
  summarize(
    power = mean(p_value < 0.05)
  )
```
Plot a.1: 
```{r}
power_df |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power of one-sample t-test vs. true mean",
    x = "True Œº",
    y = "Power (Pr reject H0: Œº = 0)"
  )

```
Comment a.1: This shows that when Œº = 0, the rejection proportion is close to 0.05, as expected for a test with Œ±=0.05. As the true Œº moves further from 0, the power increases, this illustrates that larger effect sizes yield higher power, holding sample size and variance fixed. 

(b) Average estimate of Œº hat vs. Œº
We're going to compute the mean of Œº hat across all samples, and the mean of Œº har only among samples where the null if rejected. 
```{r}
mu_est_df =
  sim_results |>
  group_by(true_mu) |>
  summarize(
    mean_mu_hat_all    = mean(mu_hat),
    mean_mu_hat_reject = mean(mu_hat[p_value < 0.05])
  )
```
Plot b.1: Average Œº hat across all samples 
```{r}
mu_est_df |>
  ggplot(aes(x = true_mu, y = mean_mu_hat_all)) +
  geom_point() +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Average estimate ŒºÃÇ across all samples",
    x = "True Œº",
    y = "Average ŒºÃÇ"
  )
```
Plot b.2: Average Œº hat among samples with rejected null 
```{r}
mu_est_df |>
  pivot_longer(
    cols = c(mean_mu_hat_all, mean_mu_hat_reject),
    names_to = "type",
    values_to = "mean_mu_hat"
  ) |>
  mutate(
    type = recode(
      type,
      mean_mu_hat_all    = "All samples",
      mean_mu_hat_reject = "Samples with p < 0.05"
    )
  ) |>
  ggplot(aes(x = true_mu, y = mean_mu_hat, color = type)) +
  geom_point() +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Average ŒºÃÇ vs. true Œº",
    x = "True Œº",
    y = "Average ŒºÃÇ",
    color = ""
  )
```
Comment b.1 & 2: Across all samples, the average estimate Œº hat lies very close to the 45-degree line, indicates that the one-sample mean is an unbiased estimator of the true mean Œº. 

While we look at b.2 to limit the range to only those samples where the null is rejected (pvalue <0.05), the average Œº hat is larger than the true Œº, especially for smaller effect sizes. When power is low, only the extreme positive fluctuations lead to rejection, so the conditional average becomes biased upward. 

As Œº increases and the test becomes more powerful, nearly all samples lead to rejection, and the conditional mean converges back toward the true mean. Thus, the average estimate based only on significant results is not equal to Œº except when power is very high.

## Problem 3

Load Data
```{r}
homicides = read_csv("homicide-data.csv")
```
Describe the raw data & create `city_state`
```{r}
homicides =
  homicides |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )
```
This dataset contains homicide records from 50 large U.S. cities. Each row represents one homicide case, includes information such as victim name, age, sex, race, date, location, and whether resulted in an arrest. 

The variable `disposition` indicates whether the homicide was solved or not. Using this variable, I then created an indicator `unsolved`, this equals to `TRUE` when the case is marked as "Closed without arrest" or "Open/No arrest." 

The variable `city_state` uniquely identifies each location. 

Now we summarize within each city: total homicides and unsolved homicides
```{r}
city_summary =
  homicides |>
  group_by(city_state) |>
  summarize(
    total = n(),
    unsolved = sum(unsolved)
  )
```
Baltimore, MD: proportion using `prop.test` and `broom:: tidy()`
```{r}
baltimore =
  city_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_test =
  prop.test(baltimore$unsolved, baltimore$total) |> tidy()

baltimore_test
```
This provides: 

estimate of unsolved proportion (`estimate`)

confidence interval (`conf.low`, `conf.high`)

p-value and other fields

Run `prop.test` for all cities using a list-column pipeline: 
```{r}
results =
  city_summary |>
  mutate(
    # run prop.test for EACH city
    test = map2(unsolved, total, prop.test),
    
    # tidy the output
    tidied = map(test, tidy)
  ) |>
  unnest(tidied)
```
Plot Estimates, confidence intervas for each city: 
```{r}
results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides with 95% CIs",
    x = "City",
    y = "Estimated proportion unsolved"
  ) + 
  theme_bw() + 
  theme(
    axis.text.y = element_text(size = 6) 
  )
```
Chicago, New Orleans, Baltimore, and etc. have unsolved rates well aboce 50%, while others have much power proportions. Cities with fewer cases show much wider uncertainity in terms of confidence interval. 

